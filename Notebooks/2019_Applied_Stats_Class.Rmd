---
title: "Applied Statistics I"
subtitle: "R for Bioscientists"
author: 
- "Jacobo de la Cuesta-Zuluaga"
date: "October 2019"
output: html_document
---

# Intro
The present notebook was prepared by Jacobo de la Cuesta-Zuluaga as part of the advanced R for bioscientists course in the Max Planck for Developmental Biology in 2019. It contains material adapted from Jukka-Pekka Verta's material for the same course in 2017 and the Data Skills for Reproducible Science course by the psyTeachR team at the University of Glasgow School of Psychology (they have a lot of great material! [check it out](https://psyteachr.github.io)).

This course assumes you are comfortable using R to wrangle, summarize and plot data. For the material used in the introduction to R course given at the MPI for Developmental Biology, [see here](https://github.com/jacodela/R4eBio).

*NOTE: You can knit this file to html to see formatted versions of the equations below (which are enclosed in `$` characters); alternatively, if you find it easier, you can hover your mouse pointer over the `$` in the code equations to see the formatted versions.* 

# Libraries
Remember that is good practice to load the libraries to be used on the top of the document
```{r}
library(tidyverse)
library(broom)
library(cowplot)
library(gvlma)
```

## Set seed 
We will be using simulations and generating random numbers. Setting a seed will make sure that we consistently get the same values.
```{r}
set.seed(2112) 
```

# Brief reminder
Before we start, let's go over the basic R commands and functions from the tidyverse. For this we will use the `iris`dataset.

To print an R object, you just type its name. Likewise, you can print the top of a data frame with the command `head()`
```{r}
# Print complete df
iris

# print top
head(iris)
```

One useful operator is the *pipe* `%>%`. It passes the content of one function or object directly to another. Think of it as saying "this *and then* that".
```{r}
# For example, this:
head(iris)

# Is the same as
iris %>% head()
```


You can select columns using multiple commands, depending on the format you want. Some of them give the same result. For example, if you want to select the `Sepal.Length` column, you can do it like this
```{r}
# Indexing by position
iris[,1]

# Indexing by column name
iris[,"Sepal.Length"]

# Using the $ operator
iris$Sepal.Length

# using pull from tidyverse
iris %>% pull(Sepal.Length)

# Using select from tidyverse
# Note that select returns a data frame instead of a vector
iris %>% select(Sepal.Length)
```

Likewise, you can filter by a condition. For example, you can select only the rows from *Iris setosa*
```{r}
# Indexing rows in base R
iris[iris$Species == "setosa",]

# Using filter from the tidyverse
iris %>% filter(Species == "setosa")
```

You can also summarize a complete data frame using the `summary()` function
```{r}
# Summary of all columns
summary(iris)
```

In addition, there are other functions that are useful to get a glimpse of your data, some of them included within the `summary` function. They include:

  * `min` Returns the minimum value of a vector
  * `max` Returns the maximum value of a vector
  * `range` Returns the minimum and maximum values of a vector
  * `mean` Returns the mean value of a vector
  * `median` Returns the median value of a vector
  * `mode` Returns the most frequent value of a vector
  * `table` Builds table of the counts of each factor levels

# Correlation
Correlation measures the strenght of the association between two continuous variables. In other words, the direction and degree to which two variables change together. Correlation analysis will tell you if one variable will tend to increase or decrease as the other variable increases or decreases. Remember that correlation analysis **does not** test causal hypotheses. 

For this section we will use the `diamonds` dataset, which contains information about the price and characteristics of 54.000 diamonds. We can test what is the association between the carat weigth of a diamond and its value

```{r}
# Print top of table
diamonds %>% head()
```


To calculate the correlation between carat weigth and price, all we use the `cor` command:
```{r, echo=TRUE}
# Calculate correlation
cor(diamonds$carat, diamonds$price, method = "pearson")
```

So what does the value of correlation coefficient (p) mean? Correlation considers the variance in variable *x*, the variance in variable *y* and their covariance (how the two vary together). Values of p always range between -1 and 1. Values near -1 indicate strong negative correlation, while values near 1 indicate strong positive correlation. When *x* and *y* are uncorrelated, their covariance (and thus p) is 0. Think of the lack of correlation as a plot of the two variables without any kind of structure, just a random cloud of points. Correlation, on the other hand, means that there is a structure to that cloud of points - maybe it's stretched on the diagonal and the could is pointing to upper right, in which case the correlation would be positive.

The associated p-value of a correlation can be calculated as follows:

```{r, echo=TRUE}
cor.test(diamonds$carat, diamonds$price, method = "pearson")
```

Note that the Pearson's correlation test assumes that *x* and *y* are normally distributed. There's also the non-parametric Spearman's rank correlation (in case of non-normality).

```{r, echo=TRUE}
cor.test(diamonds$carat, diamonds$price, method='spearman')
```

If you want to test your ability to visually assess the correlation between two variables, try [this game](http://guessthecorrelation.com/), just don't get too distracted ;)

# Linear models, ANOVA and t-tests
Before we start discussing tests, assumptions, and p-values, let's quickly go over the kinds of variables and the statistical tests most adequate for them.

## Variables and analysis types

* *Independent* variables are those you manipulate (i.e. "the thing that explains what you are working on"). These are also referred to as *predictor* or *explanatory* variables. 
* *Dependent* variables are the outcomes of your experiments (i.e. "the thing you are working on"), and are also referred to as *response* variable. 

### Continuous vs. discrete variables 

* *Continuous* variables are those that, at least theoretically, can be measured with arbitrary precision. Height, weight and temperature are continuous variables.
* *Discrete* variables cannot be measured with arbitrary precision because the data takes distinct values. Gender is a discrete variable. Discrete data can be furtherher classified into: i) *categorical* data, which is often qualitative categories, with no instrinsic order; this is also called *nominal*. Gender is a discrete, nominal variable. ii) *Ordinal* data, in which discrete values have an clear order. An example of an ordinal variable is if you perform an experiment with "low", "medium", and "high" treatment levels. 

### Variables vs analysis type

The table below is a quick guide to determine the adequate test to perform given the kind of data you have. Note that multiple tests could apply.

Predictor variable        | Method   
------------------------- | -----
Continuous                | Regression   
Categorical               | Analysis of variance (ANOVA)  
Continuous & categorical	| Analysis of covariance (ANCOVA)  

Response variable | Method
----------------- | ------
Continuous	      | Regression, ANOVA or ANCOVA 
Proportion        | Logistic regression 
Count	            | Log-linear models 


## The linear model

The General Linear Model (GLM) is a mathematical framework for expressing relationships among variables that can test linear relationships between a numerical *dependent variable* and any combination of categorical or continuous *independent variables*.

### Components

There are some mathematical conventions that you need to learn to understand the equations representing linear models. Once you understand those, learning about the GLM will get much easier.


| Component of GLM | Notation                      |
|------------------|-------------------------------|
| Dependent Variable (DV) | \( Y \)               |
| Grand Average    | \( µ \) (the Greek letter "mu")   |
| Main Effects     | \( A, B, C, \ldots \)         |
| Interactions     | \( AB, AC, BC, ABC, \ldots \) |
| Random Error     | \( S(Group) \)                |

The linear equation predicts the dependent variable ($Y$) as the sum of the grand average value of $Y$ ($\mu$, also called the intercept), the main effects of all the predictor variables ($A+B+C+ \ldots$), the interactions among all the predictor variables ($AB, AC, BC, ABC, \ldots$), and some random error ($S(Group)$). The equation for a model with two predictor variables ($A$ and $B$) and their interaction ($AB$) is written like this:

$Y$ ~ $\mu+A+B+AB+S(Group)$

Don't worry if this doesn't make sense until we walk through a concrete example.

### Simulating data from GLM

A good way to learn about linear models is to simulate data where you know exactly how the variables are related, and then analyse this simulated data to see where the parameters show up in the analysis.

We'll start with a very simple linear model that has a single categorical factor with two levels. Let's say we're predicting flowering times for high and low light conditions in a single *Arabidopsis thaliana* genotype. Average flowering time (`mu`) is 45 days, and is 7 days faster for high light than low light conditions (`effect`). 

You need to represent categorical factors with numbers. The numbers, or **coding** you choose will affect the numbers you get out of the analysis and how you need to interpret them. Here, we will use the **effect code**  with the light conditions so that high light is coded as +0.5, and low light is coded as -0.5.

Plants of the same genotype won't always respond exactly the same way. Some might flower a little faster under the same light condition than others due to random fluctuations in factors such as temperature, nutrients or other unmeassured factors. So we can add an **error term** to each condition. We can't know how much any specific replicate of the experiment will vary, but we can characterise the distribution of how much replicates differ from average and then sample from this distribution. 

Here, we'll assume the error term is sampled from a normal distribution with a standard deviation of 3 days (the mean of the error term distribution is always 0). We'll also sample 100 trials of each type, so we can see a range of variation.

So first create variables for all of the parameters that describe your data.

```{r}
# Number of repetitions per group
n_per_grp = 100

# Mean flowering time
mu = 45

# Mean difference between high and low light conditions, in days
# It is coded as negative because in our simulation, the high condition results in 
# shorter flowering time compared to the low condition
effect = -7

# Standard deviation of the error term
error_sd = 3

# Effect code
conditions = c("Low" = -0.5, "High" = 0.5)
```

Then simulate the data by creating a data table with a row for each trial and columns for the trial type and the error term (random numbers samples from a normal distribution with the SD specified by `error_sd`). For categorical variables, include both a column with the text labels (`light_cond`) and another column with the coded version (`light_cond.e`) to make it easier to check what the codings mean and to use for graphing. Calculate the dependent variable (`Flo`) as the sum of the grand mean (`mu`), the coefficient (`effect`) multiplied by the effect-coded predictor variable (`light_cond.e`), and the error term.

```{r}
dat = data.frame(light_cond = rep(names(conditions), each = n_per_grp),
  light_cond.e = rep(conditions, each = n_per_grp),
  error = rnorm(2*n_per_grp, 0, error_sd)) %>%
  mutate(Flo = mu + effect*light_cond.e + error)

dat
```

Last but not least, always plot simulated data to make sure it looks like you expect.

```{r plot-sim}
ggplot(dat, aes(light_cond, Flo)) + 
  geom_violin() +
  geom_boxplot(aes(fill = light_cond), width = 0.25, show.legend = FALSE) +
  theme_light() +
  labs(title = "Simulated data")
```

### Linear Regression
Now we can analyse the data we simulated using the function `lm()`. It takes the formula as the first argument. This is the same as the data-generating equation, but you can omit the error term (this is implied), and takes the data table as the second argument. Use the `summary()` function to see the statistical summary.

```{r}
light_lm = lm(Flo ~ light_cond.e, data = dat)

summary(light_lm)
```

Notice how the **estimate** for the `(Intercept)` is close to the value we set for `mu` and the estimate for `light_cond.e` is close to the value we set for `effect`.


*Exercise*: Change the values of `mu` and `effect`, resimulate the data, and re-run the linear model. What happens to the estimates?

### Residuals

You can use the `residuals()` function to extract the error term for each each data point. This is the Y values minus the estimates for the intercept and trial type. We’ll make a density plot of the residuals below and compare it to the normal distribution we used for the error term.

```{r res-density-plot, fig.cap="Model residuals should be approximately normally distributed for each group"}
res = residuals(light_lm)

ggplot(dat) + 
  stat_function(aes(0), color = "grey40",
                fun = dnorm, n = 101, 
                args = list(mean = 0, sd = error_sd)) +
  geom_density(aes(res, color = light_cond))
  
```

The residuals of both conditions more or less follow a normal distribution. This will become important later, because this is one of the assumptions of the linear model.


You can also compare the model residuals to the simulated error values. If the model is accurate, they should be almost identical. If the intercept estimate is slightly off, the points will be slightly above or below the black line. If the estimate for the effect of trial type is slightly off, there will be a small, systematic difference between residuals for high and low light conditions.

```{r res-err-plot, fig.cap="Model residuals should be very similar to the simulated error"}
ggplot(dat, aes(error, res, color = light_cond)) +
  geom_abline(slope = 1) +
  geom_point() +
  ylab("Model Residuals") +
  xlab("Simulated Error")
```

*Exercise*: What happens to the residuals if you fit a model that ignores light conditions (e.g., `lm(Y ~ 1, data = dat)`)?

### Predict New Values

You can use the estimates from your model to predict new data points, given values for the model parameters. For this simple example, we just need to know the light condition to make a prediction
of the flowering time.

For high light, you would predict that a new data point would be equal to the intercept estimate plus the light condition estimate multiplied by 0.5 (the effect code for high light).

```{r}
int_est = light_lm$coefficients[["(Intercept)"]]
tt_est  = light_lm$coefficients[["light_cond.e"]]
tt_code = conditions[["High"]]

new_high_Flo = int_est + tt_est * tt_code

new_high_Flo
```

You can also use the `predict()` function to do this more easily.

```{r}
predict(light_lm, data.frame(light_cond.e = 0.5))
```

### Other Coding Categorical Variables

In the example above, we used **effect coding** for light condition. You can also use **sum coding**, which assigns +1 and -1 to the levels instead of +0.5 and -0.5.  More commonly, you might want to use **treatment coding**, which assigns 0 to one level (usually a baseline or control condition) and 1 to the other level (usually a treatment or experimental condition).

Here we will add sum-coded and treatment-coded versions of `light_cond` to the dataset.

```{r}
dat = dat %>% mutate(light_cond.sum = rep(c(-1, 1), each = n_per_grp),
  light_cond.tr = rep(c(0, 1), each = n_per_grp))

dat
```

Here are the coefficients for the effect-coded version. They should be the same as those from the last analysis.
```{r}
lm(Flo ~ light_cond.e, data = dat)$coefficients
```

Here are the coefficients for the sum-coded version. This give the same results as effect coding, except the estimate for the categorical factor will be exactly half as large, as it represents the difference between each condition and the hypothetical condition of 0 (the overall mean Flo), rather than the difference between the two conditions.
```{r}
lm(Flo ~ light_cond.sum, data = dat)$coefficients
```

Here are the coefficients for the treatment-coded version. The estimate for the categorical factor will be the same as in the effect-coded version, but the intercept will increase. It will be equal to the intercept plus the estimate for condition from the sum-coded version.
```{r}
lm(Flo ~ light_cond.tr, data = dat)$coefficients
```


## Relationships among tests

You may be wondering why we started with the linear model and not with the t-test, ANOVA or ANCOVA. Well, it turns out that these tests are just special cases of the GLM. Let's see

### t-test

The t-test is just a special, limited example of a general linear model. In this case, we are comparing the difference between the means (a continuous dependent variable) between **two groups** (a categorical independent variable with two levels). 

The idea is simple: we want to know how likely it is that the samples were drawn from two populations with the same mean. In other words, are the means of the two grous equal?

One important assumpion of tests based on comparing two or more means is that the variance is equal in all compared samples. This is called **constancy of variance**. Why is this important? Because two samples can have the same mean but different variance. If you compare just the means you may conclude that the samples are identical. However, variance of the data is equally important in most contexts, so this conclusion would probably be wrong.

Let's just quickly compare the variance the two treatment groups.
```{r, echo=TRUE}
dat %>% 
  group_by(light_cond) %>% 
  summarise(var = var(Flo))
```

We can test for the equality of the variances as follows:
```{r, echo=TRUE}
var.test(Flo ~ light_cond, data = dat)
```

So, even though the estimate of the variance of each group is different, they are not *significantly* different.

One advantaje of using the t-test in R is that by default it uses a variation called the Welch's t-test for unequal variances, which is robust to deviations of the assumption of the constancy of variance. Performing the test in R is very straightforward, we just need to use the `t.test()` command and inside we just need to specify the dependent and independent variables we want to test using a formula:
```{r}
t.test(Flo ~ light_cond, data = dat)
```

*Exercise*: What happens when you use the other codings for light conditions in the t-test above? Which coding maps onto the results of the t-test best?

### ANOVA

ANOVA is also a special, limited version of the linear model. You use ANOVA when your dependent variable is categorical and you're interested in the difference in mean values between categories. You can think of ANOVA as a case of t-test when you have more than 2 groups.

Just like with the t-test, you can perform the ANOVA using the `aov()` command and a formula. **Note** that there is a related but different command named `anova()`, don't confuse them!

```{r}
light_aov = aov(Flo ~ light_cond.e, data = dat)

summary(light_aov, intercept = TRUE)
```

The easiest way to get parameters out of an analysis is to use the function `tidy()` from the `broom` package. This returns a tidy table that you can extract numbers of interest from. 

```{r}
light_broom = tidy(light_aov)
light_broom
```

**Exercise**: Compare the square root of the F-value of the linear model to the t-value from the t-tests above.
```{r}
f_val = light_broom$statistic[1]
sqrt(f_val)
```

### Understanding ANOVA

We'll walk through an example of a one-way ANOVA with the following equation:

$Y_{ij} = \mu + A_i + S(A)_{ij}$

This means that each data point ($Y_{ij}$) is predicted to be the sum of the grand mean ($\mu$), plus the effect of factor A ($A_i$), plus some residual error ($S(A)_{ij}$).

### Means, Variability, and Deviation Scores

Let's create a simple simulation function so you can quickly create a two-sample dataset with specified Ns, means, and SDs.

```{r}
two_sample = function(n = 10, m1 = 0, m2 = 0, sd1 = 1, sd2 = 1) {
  s1 = rnorm(n, m1, sd1)
  s2 = rnorm(n, m2, sd2)
  
  data.frame(
    Y = c(s1, s2),
    grp = rep(c("A", "B"), each = n)
  )
}
```

Now we will use `two_sample()` to create a dataset `dat` with N=5 per group, means of -2 and +2, and SDs of 1 and 1 (this is an effect size of d = 4).

```{r}
dat_aov = two_sample(5, -2, +2, 1, 1)
dat_aov %>% head
```

You can calculate how each data point (`Y`) deviates from the overall sample mean ($\hat{\mu}$), which is represented by the horizontal grey line below and the deviations are the vertical grey lines. You can also calculate how different each point is from its group-specific mean ($\hat{A_i}$), which are represented by the horizontal coloured lines below and the deviations are the coloured vertical lines.

```{r deviation, echo = FALSE, fig.cap="Deviations of each data point (Y) from the overall and group means"}
group_by(dat_aov, grp) %>% 
  mutate(Ymean = mean(Y)) %>%
  ungroup() %>%
  mutate(i = row_number()) %>%
  ggplot(aes(i, Y, color = grp)) +
    geom_hline(yintercept = mean(dat_aov$Y), color = "grey40") +
    geom_linerange(aes(x = i-0.025, ymin = Y, ymax = mean(dat_aov$Y)), color = "grey40") +
    geom_hline(aes(yintercept = Ymean, color = grp)) +
    geom_linerange(aes(x = i+0.025, ymin = Y, ymax = Ymean, group = i, color = grp)) +
    geom_point(size = 3) +
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank()) +
  xlab("")

```

You can use these deviations to calculate variability between groups and within groups. ANOVA tests whether the variability between groups is larger than that within groups, accounting for the number of groups and observations.

#### Decomposition matrices

We can use the estimation equations for a one-factor ANOVA to calculate the model components.

* `mu` is the overall mean 
* `a` is how different each group mean is from the overall mean
* `err` is residual error, calculated by subtracting `mu` and `a` from `Y`

This produces a *decomposition matrix*, a table with columns for `Y`, `mu`, `a`, and `err`.

```{r}
decomp = dat_aov %>% 
  select(Y, grp) %>%
  mutate(mu = mean(Y)) %>%     # calculate mu_hat
  group_by(grp) %>%
  mutate(a = mean(Y) - mu) %>% # calculate a_hat for each grp
  ungroup() %>%
  mutate(err = Y - mu - a)     # calculate residual error
```

`r knitr::kable(decomp)`

Calculate sums of squares for `mu`, `a`, and `err`.

```{r}
SS = decomp %>%
  summarise(mu = sum(mu^2),
            a = sum(a^2),
            err = sum(err^2))
```

`r knitr::kable(SS)`

If you've done everything right, `SS$mu + SS$a + SS$err` should equal the sum of squares for Y.

```{r}
SS_Y = sum(decomp$Y^2)
SS_Y == SS$mu + SS$a + SS$err
```

Divide each sum of squares by its corresponding degrees of freedom (df) to calculate mean squares. The df for `mu` is 1, the df for factor `a` is `K-1` (K is the number of groups), and the df for `err` is `N - K` (N is the number of observations).

```{r}
K = n_distinct(dat_aov$grp)
N = nrow(dat_aov)
df = c(mu = 1, a = K - 1, err = N - K)
MS = SS / df
```

`r knitr::kable(MS)`

Then calculate an F-ratio for `mu` and `a` by dividing their mean squares by the error term mean square. Get the p-values that correspond to these F-values using the `pf()` function.

```{r}
F_mu = MS$mu / MS$err
F_a  = MS$a  / MS$err
p_mu = pf(F_mu, df1 = df['mu'], df2 = df['err'], lower.tail = FALSE)
p_a  = pf(F_a,  df1 = df['a'],  df2 = df['err'], lower.tail = FALSE)
```

Put everything into a data frame to display it in the same way as the ANOVA summary function.

```{r}
my_calcs = data.frame(term = c("Intercept", "grp", "Residuals"),
  Df = df,
  SS = c(SS$mu, SS$a, SS$err),
  MS = c(MS$mu, MS$a, MS$err),
  F = c(F_mu, F_a, NA),
  p = c(p_mu, p_a, NA))
```

`r knitr::kable(my_calcs , digits = 3)`

Now run a one-way ANOVA on your results and compare it to what you obtained in your calculations.

```{r}
aov(Y ~ grp, data = dat_aov) %>% summary(intercept = TRUE)
```

# Linear regression: simple and multiple

The following section will use most of what we've seen so far, so we can do it as a series of exercises, to which I will add a couple new concepts. We will use the `msleep` dataset. It constains sleep time, weight and taxonomic classification of multiple mamals. (See `?msleep` for information about the dataset.)

## Exercise 1

Use ggplot2 to make a scatterplot that visualizes the relationship between total sleep hours and the number of REM sleep hours of the animal.
```{r}
# ggplot
```


## Exercise 2

Run a regression model that fits the number REM sleep hours by total sleep hours of the animal, and store the model object in the variable `sleep_mod`.

```{r}
# lm
```

## Exercise 3

Is the association between body weight and sleep hours significant? What is the proportion of variance in the sleep hours explained by the animal body weight?

```{r}
# summary here
```

# Exercise 4

Make a histogram of the residuals of the model using ggplot2.

```{r}
# hist
```

## Excercise 5

Sometimes you want to include other variables in your model that might have an effect on your dependent variable. These variables can be easily included in the model to perform a multiple linear regression, which is a generalization of a simple linear regression. The variables can be either continuous or categorical. When you perform a multiple linear regeression of the form `y ~ z + x` you can say that you are evaluating the association of `x` and `y` adjusting or controlling by `z`.

Fit a linear model to test the the association of total and REM hours as above, but adjusting by the (taxonomic) order. Use Summary to determine whether the association is significant.

```{r}

```

**NOTE** that this is very similar to the so called *ANCOVA* (analysis of covariance), which evaluates whether the means of a dependent continuous variable are equal across levels of a categorical independent variable, while controlling for the effects of other continuous variables that are not of primary interest.


# Assumptions of the linear model

The following section is based on the linear regression assumptions and diagnostics in R: article [from STHDA](http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/).

Once you perform a linear regression analysis, you should also verify whether it followed the assumptions of the linear model.

The GLM assumes that:

* the relationship between the dependent and independent variables is linear
* the the residuals follow a normal distribution
* the variance of the residuals is constant 
* the residuals are indepentend of each other. 

It is important to inspect the regression model to determine if there may be problems with your model and determine if the assumptions are met. Some of the potential problems you may encounter are:

* Non-linearity of the outcome - predictor relationships
* Heteroscedasticity: Non-constant variance of error terms.
* Presence of influential values in the data that can be:
    * Outliers: extreme values in the outcome (y) variable
    * High-leverage points: extreme values in the predictors (x) variable

We can use plots to assess these assumptions

## Asssessing assumptions of the linear model

First, we need to fit a linear regression model. For this, we will use the `diamonds` dataset used above. For ease of visualization, we'll randomly sample 100 observations from the data frame
```{r}
# subset of data frame
sub_diamonds = diamonds %>% sample_n(100)

# See top of table
sub_diamonds %>% head

# Plot of price of the diamonds by caratage
sub_diamonds %>% ggplot(aes(x = carat, y = price)) + geom_point()

# Fit lm
diamond_lm = lm(price ~ carat, data = sub_diamonds)

# Summary of linear model
summary(diamond_lm)
```
First, we need to understand two key concepts in regression analysis: Fitted values and residuals errors. They are very important to understand what follows

*The fitted (or predicted) values* are the y-values that you would expect for the given x-values according to the built regression model. In R, you can easily augment your data to add fitted values and residuals by using the function `augment()` from the `broom` package. The output model.diag.metrics contains several metrics useful for regression diagnostics. We’ll describe theme later.

No model is perfect, and there will always be a difference between the observed y-values and the predicted (or fitted) values given a certain x-value by the model. The differences between the observed and fitted y-values are the *residuals* of the model, represented by a vertical red lines. This means that, for a given caratage, the observed (or measured) sale price can be different from the predicted values.

```{r}
# Obtain diagnostic metrics table
diamond_diagnostics = augment(diamond_lm)
head(diamond_diagnostics)

# Scatter plot with regression line and residuals
ggplot(diamond_diagnostics, aes(x = carat, y = price)) +
  geom_point() +
  stat_smooth(method = lm, se = FALSE) +
  geom_segment(aes(xend = carat, yend = .fitted), color = "red", size = 0.3)
```

## Diagnostic plots
Regression diagnostics plots can be created using the R base function plot()
```{r}
par(mfrow = c(2, 2))
plot(diamond_lm)
```

The four plot are:

* Residuals vs Fitted: used to check the linear relationship.
* Normal Q-Q: used to examine whether the residuals are normally distributed.
* Scale-Location (or Spread-Location): used to check the homogeneity of variance of the residuals (homoscedasticity). 
* Residuals vs Leverage: Used to identify influential cases, that is extreme values that might influence the regression results when included or excluded from the analysis. 
In these plots the top 3 most extreme data points are labeled with with the row numbers in the data frame. These values might be potentially problematic, and might grant a close inspection.

Let's go over each plot

### Residuals vs Fitted
```{r}
plot(diamond_lm, 1)
```

Ideally, the residual plot will show no fitted pattern. That is, the red line should be approximately horizontal at zero. If the residual plot indicates a non-linear relationship in the data, then a simple approach is to use non-linear transformations of the predictors, such as log(x), sqrt(x) and x^2, in the regression model. 

### Normal Q-Q
```{r}
plot(diamond_lm, 2)
```

The QQ plot of residuals can be used to visually check the normality assumption. The normal probability plot of residuals should approximately follow a straight line.

### Scale-Location
```{r}
plot(diamond_lm, 3)
```

This plot shows if residuals are spread equally along the ranges of predictors. It’s good if you see a horizontal line with equally spread points. In our example, this is not the case.

### Residuals vs Leverage
```{r}
plot(diamond_lm, 5)
```

Outliers and high leverage points can be identified by inspecting the Residuals vs Leverage plot. These are values, which inclusion or exclusion can alter the results of the regression analysis and associated with a large residual. Note that not all outliers are influential in linear regression analysis. On this plot, outlying values are generally located at the upper right corner or at the lower right corner. 

Look for points outside of a dashed line, *Cook’s distance*. When the points are outside of the Cook’s distance threshold it means that they are influential to the regression results. The regression results will be altered if we exclude those cases.

## Automating the process

One of the greatest advantages of R is that is seems there is a package for everything, and obtaining diagnostics for a linear regression fit is not the exception. In this case we can use the `gvlma` function from the package of the same name

```{r}
gvlma(diamond_lm)
```

You can find a detailed explanation [here](https://ademos.people.uic.edu/Chapter12.html#38_global_test_of_model_assumptions:_gvlma()), but briefly:

* `Global Stat`: Are the relationships between the x predictors and y roughly linear?
* `Skewness`: are the data skewed positively or negatively, necessitating a transformation to meet the assumption of normality?
* `Kurtosis`: are the data kurtotic (highly peaked or very shallowly peaked), necessitating a transformation to meet the assumption of normality?
* `Link Function`: Is the dependent variable truly continuous, or categorical? Rejection of the null (p < .05) indicates that you should use an alternative form of the generalized linear model (e.g. logistic or binomial regression).
* `Heteroscedasticity`: Is the variance of your model residuals constant across the range of x (assumption of homoscedastiity)?


# Session info
It is also good to print the packages used and the characteristic of your system in case you need to reproduce the analyses in the future
```{r}
sessionInfo()
```





